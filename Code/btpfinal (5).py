# -*- coding: utf-8 -*-
"""BTPfinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16AKZ-ponSNMKccFPxU034eUVS9uHI_S5
"""

# Install necessary libraries
!pip install efficientnet_pytorch ultralytics scikit-learn tqdm joblib

from google.colab import drive
drive.mount('/content/drive')

import os
import cv2
import numpy as np
import torch
import random
from efficientnet_pytorch import EfficientNet
from torchvision import transforms
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from ultralytics import YOLO
from tqdm import tqdm
import joblib

DATA_ROOT = "/content/drive/MyDrive/goatfaces-20250225T112519Z-001/goatfaces"
MUZZLE_WEIGHTS = "/content/drive/MyDrive/best.pt"
MODEL_DIR = "/content/drive/MyDrive/goat_project2"
CROPS_ROOT = "/content/drive/MyDrive/muzzle_crops"
FEATURES_FILE = os.path.join(MODEL_DIR, "features.npz")
PIPELINE_FILE = os.path.join(MODEL_DIR, "svm_pipeline.pkl")
LABEL_ENCODER_FILE = os.path.join(MODEL_DIR, "label_encoder.pkl")

os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(CROPS_ROOT, exist_ok=True)

# CLAHE preprocessing
def clahe_preprocessing(image):
    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    return cv2.merge([clahe.apply(l), a, b])

def detect_and_crop_muzzles():
    model = YOLO(MUZZLE_WEIGHTS)
    for split in ["train", "val", "test"]:
        print(f"Processing {split} set...")
        split_path = os.path.join(DATA_ROOT, split)
        output_dir = os.path.join(CROPS_ROOT, split)
        os.makedirs(output_dir, exist_ok=True)

        for goat_dir in tqdm(os.listdir(split_path)):
            goat_path = os.path.join(split_path, goat_dir)
            if not os.path.isdir(goat_path): continue

            output_goat_dir = os.path.join(output_dir, goat_dir)
            os.makedirs(output_goat_dir, exist_ok=True)

            for img_file in os.listdir(goat_path):
                img_path = os.path.join(goat_path, img_file)
                image = cv2.imread(img_path)
                if image is None: continue

                processed = clahe_preprocessing(image)
                results = model.predict(processed, conf=0.7)

                if len(results[0].boxes.xyxy) > 0:
                    box = results[0].boxes.xyxy[0].cpu().numpy()
                    x1, y1, x2, y2 = map(int, box)
                    crop = image[y1:y2, x1:x2]
                    if crop.size > 0:
                        cv2.imwrite(os.path.join(output_goat_dir, img_file), crop)

import torch
import torch.nn.functional as F
from torchvision import transforms
from efficientnet_pytorch import EfficientNet
import numpy as np
import cv2

class EfficientNetFeatureExtractor:
    def __init__(self, model_name='efficientnet-b0', device=None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = EfficientNet.from_pretrained(model_name).to(self.device)
        self.model.eval()

        # Collect feature maps from intermediate layers
        self.feature_maps = []

        # Hook intermediate layers for feature extraction
        self.model._blocks[2].register_forward_hook(self.save_feature_map)
        self.model._blocks[4].register_forward_hook(self.save_feature_map)
        self.model._blocks[6].register_forward_hook(self.save_feature_map)

        # Preprocessing pipeline
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406],
                                 [0.229, 0.224, 0.225])
        ])

    def save_feature_map(self, module, input, output):
        self.feature_maps.append(output)

    def extract(self, image):
        self.feature_maps.clear()

        if len(image.shape) == 2:
            image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)

        image_tensor = self.transform(image).unsqueeze(0).to(self.device)
        _ = self.model(image_tensor)

        # Pool and concatenate intermediate features
        pooled_features = [torch.mean(fm, dim=(2, 3)) for fm in self.feature_maps]
        features = torch.cat(pooled_features, dim=1)

        # L2 normalize the features
        normalized_features = F.normalize(features, p=2, dim=1)

        return normalized_features.detach().cpu().numpy().flatten()

FEATURES_FILE_OSR = os.path.join(MODEL_DIR, "features_osr.npz")

def extract_features_osr():
    extractor = EfficientNetFeatureExtractor()
    features = {"train": [], "val": [], "test": []}
    raw_labels = {"train": [], "val": [], "test": []}

    # Step 1: Collect all unique labels
    label_encoder = LabelEncoder()
    all_labels = []
    label_to_dir = {}

    for split in ["train"]:
        split_path = os.path.join(CROPS_ROOT, split)
        for goat_dir in os.listdir(split_path):
            label = int(goat_dir.split("_")[-1])
            all_labels.append(label)
            label_to_dir[label] = goat_dir

    # Step 2: Skip labels 108 and 688
    skipped_labels = [108, 688]
    print(f"Skipping classes: {skipped_labels}")

    unique_labels = list(set(all_labels))
    train_labels = [l for l in unique_labels if l not in skipped_labels]
    label_encoder.fit(train_labels)
    joblib.dump(label_encoder, LABEL_ENCODER_FILE)

    # Step 3: Extract features
    for split in ["train", "val", "test"]:
        split_path = os.path.join(CROPS_ROOT, split)
        for goat_dir in tqdm(os.listdir(split_path), desc=f"Extracting {split}"):
            label = int(goat_dir.split("_")[-1])

            # Skip excluded classes for training only
            if split == "train" and label in skipped_labels:
                continue

            if split == "train" and label not in train_labels:
                continue

            if split != "train" and label not in unique_labels:
                continue  # avoid unseen class errors

            try:
                encoded_label = label_encoder.transform([label])[0]
            except:
                continue  # unknown class in val/test, we ignore during training

            goat_path = os.path.join(split_path, goat_dir)
            for img_file in os.listdir(goat_path):
                img_path = os.path.join(goat_path, img_file)
                image = cv2.imread(img_path)
                if image is None: continue

                try:
                    feature = extractor.extract(image)
                    features[split].append(feature)
                    raw_labels[split].append(encoded_label)
                except Exception as e:
                    print(f"Error processing {img_path}: {str(e)}")

    np.savez(FEATURES_FILE_OSR,
             train_features=features["train"], train_labels=raw_labels["train"],
             val_features=features["val"], val_labels=raw_labels["val"],
             test_features=features["test"], test_labels=raw_labels["test"])

import numpy as np
import pandas as pd
import joblib
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC

# Constants (make sure these are defined in your environment or replace them with actual paths)

def train_svm_with_cv_osr():
    # Load features and labels
    data = np.load(FEATURES_FILE_OSR)
    X_train = data["train_features"]
    y_train = data["train_labels"]
    X_val = data["val_features"]
    y_val = data["val_labels"]
    X_test = data["test_features"]
    y_test = data["test_labels"]

    # Combine train and val sets for cross-validation
    X_cv = np.concatenate([X_train, X_val])
    y_cv = np.concatenate([y_train, y_val])

    # Define the pipeline
    pipeline = Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA(svd_solver='full')),
        ("svm", SVC(kernel="rbf", probability=True))
    ])

    # Define hyperparameter grid
    param_grid = {
        "pca__n_components": [50, 100, 120],
        "svm__C": [0.1, 1, 10],
        "svm__gamma": ["scale", 0.1, 1]
    }

    # Cross-validation setup
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    grid = GridSearchCV(
        pipeline,
        param_grid,
        cv=cv,
        n_jobs=-1,
        verbose=2,
        error_score='raise'
    )

    # Fit the grid search
    grid.fit(X_cv, y_cv)

    # Print best result
    print(f"Best CV Accuracy: {grid.best_score_:.4f}")
    print(f"Best Parameters: {grid.best_params_}")

    # Save the best model
    joblib.dump(grid.best_estimator_, PIPELINE_FILE)

    # Save full grid search results to CSV
    results_df = pd.DataFrame(grid.cv_results_)
    selected_cols = [
        "param_pca__n_components",
        "param_svm__C",
        "param_svm__gamma",
        "mean_test_score",
        "std_test_score",
        "rank_test_score"
    ]
    results_df = results_df[selected_cols].sort_values(by="rank_test_score")
    results_df.to_csv("svm_grid_search_results.csv", index=False)

    print("\nGrid Search Results (sorted by best score):")
    print(results_df.to_string(index=False))

    # Final evaluation on train, val, and test sets
    print("\nFinal Evaluation:")
    for name, X, y in [
        ("Train", X_train, y_train),
        ("Validation", X_val, y_val),
        ("Test", X_test, y_test)
    ]:
        acc = grid.best_estimator_.score(X, y)
        print(f"{name} Accuracy: {acc:.4f}")

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import numpy as np
import joblib
import pandas as pd

def evaluate_model(y_true, y_pred, y_proba):
    return {
        "accuracy": accuracy_score(y_true, y_pred),
        "precision": precision_score(y_true, y_pred, average="weighted"),
        "recall": recall_score(y_true, y_pred, average="weighted"),
        "f1": f1_score(y_true, y_pred, average="weighted"),
        "auc": roc_auc_score(y_true, y_proba, multi_class="ovr", average="weighted")
    }

def cross_validate_svm_with_metrics():
    # Load features and labels
    data = np.load(FEATURES_FILE_OSR)
    X = np.concatenate([data["train_features"], data["val_features"]])
    y = np.concatenate([data["train_labels"], data["val_labels"]])

    # Best hyperparameters from grid search
    best_params = {
        "pca__n_components": 100,
        "svm__C": 10,
        "svm__gamma": "scale"
    }

    # Setup pipeline
    pipeline = Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA(n_components=best_params["pca__n_components"], svd_solver='full')),
        ("svm", SVC(C=best_params["svm__C"], gamma=best_params["svm__gamma"], probability=True))
    ])

    # Cross-validation
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    fold_metrics = []

    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        pipeline.fit(X_train, y_train)

        y_pred = pipeline.predict(X_val)
        y_proba = pipeline.predict_proba(X_val)

        metrics = evaluate_model(y_val, y_pred, y_proba)
        metrics["fold"] = fold
        print(f"\nFold {fold} Metrics:")
        for key, val in metrics.items():
            if key != "fold":
                print(f"{key.capitalize()}: {val:.4f}")
        fold_metrics.append(metrics)

    # Create DataFrame and save to CSV
    df_metrics = pd.DataFrame(fold_metrics)
    avg_metrics = df_metrics.drop(columns="fold").mean().to_dict()
    avg_metrics["fold"] = "avg"
    df_metrics = pd.concat([df_metrics, pd.DataFrame([avg_metrics])], ignore_index=True)

    # Save to CSV
    columns_order = ["fold"] + [col for col in df_metrics.columns if col != "fold"]
    df_metrics = df_metrics[columns_order]
    df_metrics.to_csv("svm_crossval_metrics.csv", index=False)

    print("\nAverage Cross-Validation Metrics:")
    print(pd.DataFrame([avg_metrics]).drop(columns="fold").to_string(index=False, float_format="%.4f"))

    # Optional: Save final model trained on full data
    pipeline.fit(X, y)
    joblib.dump(pipeline, PIPELINE_FILE)

    return df_metrics

extract_features_osr()

train_svm_with_cv_osr()

cross_validate_svm_with_metrics()

from matplotlib import pyplot as plt
import seaborn as sns
_df_4.groupby('fold').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
_df_0['index'].plot(kind='hist', bins=20, title='index')
plt.gca().spines[['top', 'right',]].set_visible(False)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
import joblib

def plot_per_class_accuracy(model, X, y, label_encoder):
    y_pred = model.predict(X)

    unique_classes = np.unique(y)
    class_accuracies = []

    for cls in unique_classes:
        idxs = (y == cls)
        acc = accuracy_score(y[idxs], y_pred[idxs])
        class_accuracies.append(acc)

    # Convert encoded labels back to goat tag numbers using the label encoder
    class_names = label_encoder.inverse_transform(unique_classes)

    # Plot
    plt.figure(figsize=(14, 6))
    plt.bar(class_names.astype(str), class_accuracies, color='skyblue')
    plt.xticks(rotation=45, ha='right')
    plt.ylim(0, 1.0)
    plt.title("Per-Class Accuracy (Seen Classes)")
    plt.xlabel("Goat Tag (Class ID)")
    plt.ylabel("Accuracy")
    plt.tight_layout()
    plt.grid(True, axis='y')
    plt.show()

# # Load trained model and test data
# PIPELINE_FILE
# FEATURES_FILE_OSR
# LABEL_ENCODER_FILE

model = joblib.load(PIPELINE_FILE)
label_encoder = joblib.load(LABEL_ENCODER_FILE)

data = np.load(FEATURES_FILE_OSR)
X_test = data["test_features"]
y_test = data["test_labels"]  # This contains encoded labels like 0, 1, 2...

plot_per_class_accuracy(model, X_test, y_test, label_encoder)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import joblib

def plot_seen_class_confusion_matrix():
    # Load model
    model = joblib.load(PIPELINE_FILE)

    # Load test data
    data = np.load(FEATURES_FILE_OSR)
    X_test = data["test_features"]
    y_test = data["test_labels"]

    # Get predictions
    y_pred = model.predict(X_test)

    # Compute confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    unique_labels = np.unique(np.concatenate([y_test, y_pred]))

    # Plot heatmap
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=unique_labels,
                yticklabels=unique_labels)
    plt.title("Confusion Matrix - Seen Classes (Test Set)")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.xticks(rotation=45)
    plt.yticks(rotation=45)
    plt.tight_layout()
    plt.show()

# Call this function after training and saving your model
plot_seen_class_confusion_matrix()

TEST_FOLDER="/content/drive/MyDrive/muzzle_crops/test"

FULL_TEST_FOLDER = "/content/drive/MyDrive/goatfaces-20250225T112519Z-001/goatfaces/test"

from collections import Counter
import os
import cv2
import numpy as np
import pandas as pd
import joblib
from tqdm import tqdm

def calculate_folder_accuracy_with_threshold(threshold=0.8, save_csv=True):
    pipeline = joblib.load(PIPELINE_FILE)
    label_encoder = joblib.load(LABEL_ENCODER_FILE)
    extractor = EfficientNetFeatureExtractor()

    total_seen = 0
    correct_seen = 0
    misclassified_seen = []

    total_unseen = 0
    correctly_flagged_unknown = 0
    wrongly_classified_unseen = []

    stats = Counter()

    for class_folder in tqdm(os.listdir(TEST_FOLDER), desc="Processing classes"):
        class_path = os.path.join(TEST_FOLDER, class_folder)
        if not os.path.isdir(class_path):
            continue

        true_label = int(class_folder.split("_")[-1])
        is_seen = true_label in label_encoder.classes_

        if is_seen:
            true_label_encoded = label_encoder.transform([true_label])[0]

        for img_file in tqdm(os.listdir(class_path), desc=f"Class {true_label}", leave=False):
            img_path = os.path.join(class_path, img_file)
            image = cv2.imread(img_path)
            if image is None:
                print(f"Could not read image: {img_path}")
                continue

            try:
                features = extractor.extract(image)
            except Exception as e:
                print(f"Error processing {img_path}: {str(e)}")
                continue

            probas = pipeline.predict_proba([features])[0]
            max_prob = np.max(probas)

            if max_prob < threshold:
                # Predict as unknown
                if is_seen:
                    total_seen += 1
                    misclassified_seen.append({
                        'path': img_path,
                        'true': true_label,
                        'predicted': 'Unknown',
                        'confidence': max_prob
                    })
                else:
                    correctly_flagged_unknown += 1
                stats['predicted_unknown'] += 1
            else:
                predicted_idx = np.argmax(probas)
                predicted_label = label_encoder.inverse_transform([predicted_idx])[0]
                stats['predicted_known'] += 1

                if is_seen:
                    total_seen += 1
                    if predicted_idx == true_label_encoded:
                        correct_seen += 1
                    else:
                        misclassified_seen.append({
                            'path': img_path,
                            'true': true_label,
                            'predicted': predicted_label,
                            'confidence': max_prob
                        })
                else:
                    total_unseen += 1
                    wrongly_classified_unseen.append({
                        'path': img_path,
                        'true': true_label,
                        'predicted': predicted_label,
                        'confidence': max_prob
                    })

    total_unseen += correctly_flagged_unknown  # All unseen images processed

    seen_accuracy = correct_seen / total_seen if total_seen else 0
    unseen_detection_accuracy = correctly_flagged_unknown / total_unseen if total_unseen else 0

    print(f"\n---- Open-Set Evaluation ----")
    print(f"Threshold: {threshold}")
    print(f"Seen Accuracy: {100 * seen_accuracy:.2f}% ({correct_seen}/{total_seen})")
    print(f"Unseen Detection Accuracy: {100 * unseen_detection_accuracy:.2f}% ({correctly_flagged_unknown}/{total_unseen})")

    # Add full image paths from FULL_TEST_FOLDER
    if misclassified_seen:
        for item in misclassified_seen:
            class_folder = f"class_{item['true']}"
            filename = os.path.basename(item['path'])
            original_img_path = os.path.join(FULL_TEST_FOLDER, class_folder, filename)
            item['original_image_path'] = original_img_path

    if wrongly_classified_unseen:
        for item in wrongly_classified_unseen:
            class_folder = f"class_{item['true']}"
            filename = os.path.basename(item['path'])
            original_img_path = os.path.join(FULL_TEST_FOLDER, class_folder, filename)
            item['original_image_path'] = original_img_path

    # Save results
    if save_csv:
        pd.DataFrame(misclassified_seen).to_csv("misclassified_seen.csv", index=False)
        pd.DataFrame(wrongly_classified_unseen).to_csv("unseen_misclassifications.csv", index=False)
        print("\nMisclassifications saved to CSV with original image paths.")

    return {
        'seen_accuracy': seen_accuracy,
        'unseen_detection_accuracy': unseen_detection_accuracy,
        'total_seen': total_seen,
        'total_unseen': total_unseen
    }

calculate_folder_accuracy_with_threshold(0.66,True)

from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_fscore_support
from sklearn.metrics import precision_recall_curve, auc
import matplotlib.pyplot as plt

def compute_osr_metrics(threshold=0.8):
    pipeline = joblib.load(PIPELINE_FILE)
    label_encoder = joblib.load(LABEL_ENCODER_FILE)
    extractor = EfficientNetFeatureExtractor()

    y_true = []     # 1 for seen, 0 for unseen
    y_scores = []   # max probability/confidence scores

    for class_folder in tqdm(os.listdir(TEST_FOLDER), desc="Processing for OSR metrics"):
        class_path = os.path.join(TEST_FOLDER, class_folder)
        if not os.path.isdir(class_path):
            continue

        true_label = int(class_folder.split("_")[-1])
        is_seen = true_label in label_encoder.classes_

        if is_seen:
            true_label_encoded = label_encoder.transform([true_label])[0]

        for img_file in os.listdir(class_path):
            img_path = os.path.join(class_path, img_file)
            image = cv2.imread(img_path)
            if image is None:
                continue

            try:
                features = extractor.extract(image)
            except:
                continue

            probas = pipeline.predict_proba([features])[0]
            max_prob = np.max(probas)

            y_scores.append(max_prob)
            y_true.append(1 if is_seen else 0)

    # Metrics
    auroc = roc_auc_score(y_true, y_scores)
    aupr = average_precision_score(y_true, y_scores)

    # Convert scores to predictions using threshold
    y_pred = [1 if s >= threshold else 0 for s in y_scores]
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')

    # OSCR (simplified: AUROC between accuracy on knowns vs confidence on unknowns)
    # Sort by confidence, calculate TPR (seen correctly accepted) vs FPR (unseen falsely accepted)
    sorted_indices = np.argsort(y_scores)[::-1]
    y_true = np.array(y_true)[sorted_indices]
    oscr = 0
    correct = 0
    total_seen = sum(y_true)
    total_unseen = len(y_true) - total_seen

    if total_seen > 0 and total_unseen > 0:
        tpr = []
        fpr = []
        for i in range(len(y_true)):
            if y_true[i] == 1:
                correct += 1
            tpr.append(correct / total_seen)
            fpr.append((i + 1 - correct) / total_unseen)
        oscr = auc(fpr, tpr)

    # Print Results
    print(f"\n--- Open-Set Recognition Metrics ---")
    print(f"AUROC:  {auroc:.4f}")
    print(f"AUPR:   {aupr:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1 Score:  {f1:.4f}")
    print(f"OSCR:      {oscr:.4f}")

    # Optional: plot Precision-Recall Curve
    precs, recalls, _ = precision_recall_curve(y_true, y_scores)
    plt.figure(figsize=(8, 6))
    plt.plot(recalls, precs, label=f'AUPR = {aupr:.4f}')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve (Seen vs Unseen)')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

compute_osr_metrics(threshold=0.66)

def plot_threshold_vs_accuracy():
    import matplotlib.pyplot as plt

    thresholds = [round(x * 0.05, 2) for x in range(1, 21)]  # 0.05 to 1.0
    seen_accuracies = []
    unseen_accuracies = []

    for thresh in thresholds:
        print(f"\nEvaluating for threshold = {thresh}")
        results = calculate_folder_accuracy_with_threshold(threshold=thresh, save_csv=False)
        seen_accuracies.append(results['seen_accuracy'])
        unseen_accuracies.append(results['unseen_detection_accuracy'])

    # Plot
    plt.figure(figsize=(10, 6))
    plt.plot(thresholds, seen_accuracies, label='Seen Accuracy', marker='o')
    plt.plot(thresholds, unseen_accuracies, label='Unseen Detection Accuracy', marker='x')
    plt.title("Threshold vs Accuracy")
    plt.xlabel("Threshold")
    plt.ylabel("Accuracy")
    plt.ylim(0, 1.05)
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

plot_threshold_vs_accuracy()

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset, DataLoader
from itertools import combinations, product
import matplotlib.pyplot as plt
import pandas as pd
import random

# === Dataset ===
class PairDataset(Dataset):
    def __init__(self, features, pairs, labels):
        self.pairs = pairs
        self.labels = labels
        self.features = features
        self.scaler = StandardScaler()
        self.features = self.scaler.fit_transform(self.features)

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        i, j = self.pairs[idx]
        f1 = self.features[i]
        f2 = self.features[j]
        pair_feature = np.abs(f1 - f2)
        return torch.tensor(pair_feature, dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)

# === MLP Model ===
class MLPVerifier(nn.Module):
    def __init__(self, input_dim, hidden1, hidden2, dropout1, dropout2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden1),
            nn.BatchNorm1d(hidden1),
            nn.ReLU(),
            nn.Dropout(dropout1),
            nn.Linear(hidden1, hidden2),
            nn.BatchNorm1d(hidden2),
            nn.ReLU(),
            nn.Dropout(dropout2),
            nn.Linear(hidden2, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.net(x)

# === Pair Generation ===
def generate_limited_pairs(features, labels, num_pairs=500):
    genuine_pairs, impostor_pairs = [], []
    label_to_indices = {}

    for idx, label in enumerate(labels):
        label_to_indices.setdefault(label, []).append(idx)

    for indices in label_to_indices.values():
        if len(indices) >= 2:
            all_combinations = list(combinations(indices, 2))
            random.shuffle(all_combinations)
            genuine_pairs.extend(all_combinations)

    random.shuffle(genuine_pairs)
    genuine_pairs = genuine_pairs[:num_pairs]

    all_labels = list(label_to_indices.keys())
    for i in range(len(all_labels)):
        for j in range(i + 1, len(all_labels)):
            for idx1 in label_to_indices[all_labels[i]]:
                for idx2 in label_to_indices[all_labels[j]]:
                    impostor_pairs.append((idx1, idx2))

    random.shuffle(impostor_pairs)
    impostor_pairs = impostor_pairs[:num_pairs]

    return genuine_pairs + impostor_pairs, [1] * num_pairs + [0] * num_pairs

# === Train-Eval Wrapper ===
def train_and_eval(hidden1, hidden2, dropout1, dropout2, lr, weight_decay):
    # Load data
    data = np.load("/content/drive/MyDrive/goat_project2/features_osr.npz", allow_pickle=True)
    train_features = data["train_features"]
    train_labels = data["train_labels"]
    test_features = data["test_features"]
    test_labels = data["test_labels"]

    # Training pairs
    train_pairs, train_pair_labels = generate_limited_pairs(train_features, train_labels, num_pairs=3000)
    train_dataset = PairDataset(train_features, train_pairs, train_pair_labels)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

    # Model
    input_dim = train_features.shape[1]
    model = MLPVerifier(input_dim, hidden1, hidden2, dropout1, dropout2)

    # Compute pos_weight
    num_pos = sum(train_pair_labels)
    num_neg = len(train_pair_labels) - num_pos
    pos_weight = torch.tensor(num_neg / num_pos, dtype=torch.float32)

    # Loss, optimizer, scheduler
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)

    # Training loop
    best_loss = float('inf')
    wait = 0
    for epoch in range(100):
        model.train()
        epoch_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            logits = model(batch_x).squeeze()
            loss = criterion(logits, batch_y)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        epoch_loss /= len(train_loader)
        scheduler.step(epoch_loss)

        if epoch_loss < best_loss:
            best_loss = epoch_loss
            wait = 0
        else:
            wait += 1
            if wait >= 7:
                break

    # Evaluation
    test_pairs, test_pair_labels = generate_limited_pairs(test_features, test_labels, num_pairs=500)
    test_dataset = PairDataset(test_features, test_pairs, test_pair_labels)
    test_loader = DataLoader(test_dataset, batch_size=32)

    model.eval()
    with torch.no_grad():
        X_test, y_test = [], []
        for x, y in test_loader:
            X_test.append(x)
            y_test.append(y)
        X_test = torch.cat(X_test)
        y_test = torch.cat(y_test)

        logits = model(X_test).squeeze()
        y_pred = torch.sigmoid(logits).numpy()

    auc = roc_auc_score(y_test.numpy(), y_pred)
    return auc

# === Hyperparameter Tuning ===
def hyperparameter_search():
    param_grid = {
        "hidden1": [128, 256],
        "hidden2": [64, 128],
        "dropout1": [0.3, 0.4],
        "dropout2": [0.2, 0.3],
        "lr": [0.001, 0.0005],
        "weight_decay": [1e-4, 1e-5]
    }

    all_combos = list(product(
        param_grid["hidden1"],
        param_grid["hidden2"],
        param_grid["dropout1"],
        param_grid["dropout2"],
        param_grid["lr"],
        param_grid["weight_decay"]
    ))

    results = []
    for i, (h1, h2, d1, d2, lr, wd) in enumerate(all_combos):
        print(f"\nRunning combo {i+1}/{len(all_combos)}: h1={h1}, h2={h2}, d1={d1}, d2={d2}, lr={lr}, wd={wd}")
        auc = train_and_eval(h1, h2, d1, d2, lr, wd)
        results.append({
            "hidden1": h1,
            "hidden2": h2,
            "dropout1": d1,
            "dropout2": d2,
            "lr": lr,
            "weight_decay": wd,
            "AUC": auc
        })
        print(f"AUC: {auc:.4f}")

    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values(by="AUC", ascending=False)
    results_df.to_csv("mlp_hyperparameter_results.csv", index=False)
    print("\n=== Hyperparameter Tuning Results ===")
    print(results_df.to_string(index=False))

# Run the tuning
hyperparameter_search()

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import classification_report, roc_auc_score, accuracy_score
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset, DataLoader
from itertools import combinations
import matplotlib.pyplot as plt
import pandas as pd
import random

# === Dataset ===
class PairDataset(Dataset):
    def __init__(self, features, pairs, labels):
        self.pairs = pairs
        self.labels = labels
        self.features = features
        self.scaler = StandardScaler()
        self.features = self.scaler.fit_transform(self.features)

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        i, j = self.pairs[idx]
        f1 = self.features[i]
        f2 = self.features[j]
        pair_feature = np.abs(f1 - f2)
        return torch.tensor(pair_feature, dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)

# === MLP Model ===
class MLPVerifier(nn.Module):
    def __init__(self, input_dim, hidden1, hidden2, dropout1, dropout2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden1),
            nn.BatchNorm1d(hidden1),
            nn.ReLU(),
            nn.Dropout(dropout1),
            nn.Linear(hidden1, hidden2),
            nn.BatchNorm1d(hidden2),
            nn.ReLU(),
            nn.Dropout(dropout2),
            nn.Linear(hidden2, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.net(x)

# === Pair Generation ===
def generate_limited_pairs(features, labels, num_pairs=500):
    genuine_pairs, impostor_pairs = [], []
    label_to_indices = {}

    for idx, label in enumerate(labels):
        label_to_indices.setdefault(label, []).append(idx)

    for indices in label_to_indices.values():
        if len(indices) >= 2:
            all_combinations = list(combinations(indices, 2))
            random.shuffle(all_combinations)
            genuine_pairs.extend(all_combinations)

    random.shuffle(genuine_pairs)
    genuine_pairs = genuine_pairs[:num_pairs]

    all_labels = list(label_to_indices.keys())
    for i in range(len(all_labels)):
        for j in range(i + 1, len(all_labels)):
            for idx1 in label_to_indices[all_labels[i]]:
                for idx2 in label_to_indices[all_labels[j]]:
                    impostor_pairs.append((idx1, idx2))

    random.shuffle(impostor_pairs)
    impostor_pairs = impostor_pairs[:num_pairs]

    return genuine_pairs + impostor_pairs, [1] * num_pairs + [0] * num_pairs

# === Train-Eval Wrapper ===
def train_and_eval(hidden1, hidden2, dropout1, dropout2, lr, weight_decay):
    # Load data
    data = np.load("/content/drive/MyDrive/goat_project2/features_osr.npz", allow_pickle=True)
    train_features = data["train_features"]
    train_labels = data["train_labels"]
    test_features = data["test_features"]
    test_labels = data["test_labels"]

    # Training pairs
    train_pairs, train_pair_labels = generate_limited_pairs(train_features, train_labels, num_pairs=3000)
    train_dataset = PairDataset(train_features, train_pairs, train_pair_labels)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

    # Model
    input_dim = train_features.shape[1]
    model = MLPVerifier(input_dim, hidden1, hidden2, dropout1, dropout2)

    # Compute pos_weight
    num_pos = sum(train_pair_labels)
    num_neg = len(train_pair_labels) - num_pos
    pos_weight = torch.tensor(num_neg / num_pos, dtype=torch.float32)

    # Loss, optimizer, scheduler
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)

    # Training loop
    best_loss = float('inf')
    wait = 0
    for epoch in range(100):
        model.train()
        epoch_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            logits = model(batch_x).squeeze()
            loss = criterion(logits, batch_y)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        epoch_loss /= len(train_loader)
        scheduler.step(epoch_loss)

        if epoch_loss < best_loss:
            best_loss = epoch_loss
            wait = 0
        else:
            wait += 1
            if wait >= 7:
                break

    # Evaluation
    test_pairs, test_pair_labels = generate_limited_pairs(test_features, test_labels, num_pairs=500)
    test_dataset = PairDataset(test_features, test_pairs, test_pair_labels)
    test_loader = DataLoader(test_dataset, batch_size=32)

    model.eval()
    with torch.no_grad():
        X_test, y_test = [], []
        for x, y in test_loader:
            X_test.append(x)
            y_test.append(y)
        X_test = torch.cat(X_test)
        y_test = torch.cat(y_test)

        logits = model(X_test).squeeze()
        y_prob = torch.sigmoid(logits).numpy()
        y_pred = (y_prob >= 0.5).astype(int)

    # AUC and Accuracy
    auc = roc_auc_score(y_test.numpy(), y_prob)
    acc = accuracy_score(y_test.numpy(), y_pred)

    # Precision, Recall, F1-Score, etc.
    class_report = classification_report(y_test.numpy(), y_pred, target_names=["Impostor", "Genuine"], output_dict=True)

    # Access precision, recall, f1 for both classes explicitly
    precision = class_report["Genuine"]["precision"]
    recall = class_report["Genuine"]["recall"]
    f1 = class_report["Genuine"]["f1-score"]

    print(f"\nAUC: {auc:.6f}")
    print(f"Accuracy: {acc:.6f}")
    print(f"Precision: {precision:.6f}")
    print(f"Recall: {recall:.6f}")
    print(f"F1-Score: {f1:.6f}")

    return auc, acc, precision, recall, f1

# === Run Specific Hyperparameters ===
if __name__ == "__main__":
    # Given hyperparameters
    h1 = 128
    h2 = 128
    d1 = 0.3
    d2 = 0.2
    lr = 0.001
    wd = 0.0001

    print(f"Running with: hidden1={h1}, hidden2={h2}, dropout1={d1}, dropout2={d2}, lr={lr}, weight_decay={wd}")
    auc, acc, precision, recall, f1 = train_and_eval(h1, h2, d1, d2, lr, wd)